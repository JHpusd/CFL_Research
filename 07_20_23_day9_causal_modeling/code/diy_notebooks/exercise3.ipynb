{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cheninstitutecaltech/Caltech_DATASAI_Neuroscience_23/blob/main/07_20_23_day9_causal_modeling/code/solutions/exercise3.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal structure discovery: HCP resting-state fMRI data\n",
    "Authors: Iman Wahle and Frederick Eberhardt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the workflow explored in exercises 1 and 2 to \n",
    "infer a causal graph over parcels in the human brain from resting-state fMRI\n",
    "activity as was done in [Dubois et al. 2017](https://www.biorxiv.org/content/10.1101/214486v1.full.pdf). The data we are working with was collected through the [Human\n",
    "Connectome Project](https://www.humanconnectome.org/study/hcp-young-adult/project-protocol/resting-state-fmri) (HCP). In particular, we will be working with \"Dataset 1\"\n",
    "specified in Dubois et al., which consists of 11 files of mean-centered \n",
    "resting-state samples from distinct sets of 80 subjects. Each file includes \n",
    "5440 samples (68 per subject) of activity over 110 parcels in the brain. \n",
    "The parcellation used here is the Harvard-Oxford atlas. The value for each \n",
    "parcel is set to be the average activity across all voxels within the parcel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab setup\n",
    "!git clone https://github.com/eberharf/fges-py.git\n",
    "!pip install -q corner dill sortedcontainers gdown\n",
    "!cd fges-py\n",
    "import gdown\n",
    "gdown.download_folder(url='https://drive.google.com/drive/folders/1vX4ZP63YTXKZSKIouwkXb-f--JNlfWoy',\n",
    "                      output='data', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_context('talk')\n",
    "import sys\n",
    "sys.path.append('fges-py')\n",
    "from SEMScore import *\n",
    "from fges import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, write a function `load_data` that takes a `file_id` int as\n",
    "an argument and returns a matrix of shape n_samples x n_parcels.\n",
    "\n",
    "The data is stored at 'data/HCPcombined_HO110_25_GM_Finn_noTsmooth_RL_step35_nSub80_{}.tsv',\n",
    "where `{}` should be replaced by the `file_id`. tsv files can be loaded\n",
    "using `np.loadtxt`. The first row in each file contains column headers and\n",
    "can be skipped. Entries in the file are separated by tabs.\n",
    "\n",
    "Load in the data for `file_id = 1`.\n",
    "\n",
    "A list of variable names is saved in 'data/parcel_labels.npy'. Load this in as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, make sure the data matches your expectations:\n",
    "\n",
    "1. Print out the shape of the data and the names of the variables included\n",
    "   - make note of how the parcels are ordered from the list of parcel names\n",
    "2. Construct a [corner plot](https://corner.readthedocs.io/en/latest/pages/quickstart/)\n",
    "   for five parcels (doing this for all parcels will take a really long time)\n",
    "3. Plot the correlation matrix over all parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect data shape and variable labels\n",
    "# confirm that the number of samples and number of parcels match what we\n",
    "# expect, and that the data matrix is formatted as (n_samples, n_parcels)\n",
    "\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a corner plot of the first few parcels\n",
    "from corner import corner\n",
    "\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix between all 110 parcels (zero out the diagonal). \n",
    "\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What structure do you see in the correlation matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the FGES algorithm\n",
    "\n",
    "Since we are working with a relatively large dataset here, we will use an\n",
    "alternative to the PC algorithm called Fast Greedy Equivalence Search (FGES), \n",
    "which is optimized for large numbers of variables.\n",
    "The implementation we will use can be found [here](https://github.com/eberharf/fges-py),\n",
    "and details about the algorithm can be found in Ramsey et al. 2016. \n",
    "\n",
    "The following function specifies an FGES object that infers edges across our\n",
    "variables from the data provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_edges(data, s=8):\n",
    "    '''\n",
    "    Arguments:\n",
    "        data : an n_samples x n_nodes array\n",
    "        s : sparsity parameter for FGES (default = 8 as was used in Dubois et al.)\n",
    "    Returns:\n",
    "        edges : a list of tuples, where each tuple (i,j) represents an edge \n",
    "                found between node i and node j\n",
    "        fges_result : dict of results from fges.search() (needed for estimating\n",
    "                      the correlation matrix later on)\n",
    "    '''\n",
    "\n",
    "    # FGES takes a score function that depends on the data and a user-determined\n",
    "    # sparsity level (penalty discount)\n",
    "    score = SEMBicScore(penalty_discount=s, dataset=data)\n",
    "\n",
    "    # run FGES\n",
    "    fges = FGES(range(data.shape[1]), score, filename=data)\n",
    "    fges_result = fges.search()\n",
    "    edges = fges_result['graph'].edges()\n",
    "    return edges, fges_result\n",
    "\n",
    "edges,fges_result = infer_edges(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of specifying the PDAG as an n_nodes x n_nodes matrix\n",
    "like those we worked with in the previous exercises, this package specifies \n",
    "adjacencies as a list of tuples (`edges`), where each tuple $(i,j)$ included in\n",
    "the list indicates a directed edge from $i$ into $j$. An undirected edge is\n",
    "represented by including both $(i,j)$ and $(j,i)$ in the list.\n",
    "\n",
    "For now, we are just interested in node adjacencies (without orientation \n",
    "information). Write a function `fges_edges_to_mat` that takes as input:\n",
    "\n",
    "- `edges` : a list of edge tuples\n",
    "- `n_nodes` : the total number of variables in our graph\n",
    "\n",
    "The function should return an `n_nodes` x `n_nodes` numpy array, where entries\n",
    "$(i,j)$ and $(j,i)$ are both set to 1 if there is an edge between node $i$ and\n",
    "node $j$ and are 0 otherwise.\n",
    "\n",
    "Use your function to convert the `edges` list constructed above to an array \n",
    "`adj_mat`. Visualize the resulting array using `plt.imshow` (make sure to \n",
    "label the parcel names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of edges that fges returns to an adjacency matrix where entries\n",
    "# (i,j) and (j,i) are 1 if there is an edge between node i and node j and 0 otherwise\n",
    "def fges_edges_to_mat(edges, n_nodes):\n",
    "    \n",
    "    # add code here\n",
    "    \n",
    "    pass\n",
    "\n",
    "adj_mat = fges_edges_to_mat(edges, data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize adjacency matrix found by algorithm. What structure do you see? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize adjacency matrix found by algorithm. What structure do you see?\n",
    "\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in the previous workflows, we had to 1) convert the PDAG to a DAG,\n",
    "2) estimate the weights connection weights and residuals, and 3) compute\n",
    "the resulting correlation matrix of the estimated graph, `fges-py` provides\n",
    "a class called `SemEstimator` that will do this all for us. The following function\n",
    "does so and returns the numpy array `est_corr` that is the correlation matrix\n",
    "from the inferred graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SemEstimator import SemEstimator\n",
    "\n",
    "def estimate_corr(data, fges_result):\n",
    "    '''\n",
    "    Arguments:\n",
    "        data : an n_samples x n_nodes numpy array\n",
    "        fges_result : a dict of results returned by fges.search()\n",
    "    Returns:\n",
    "        est_corr : an n_nodes x n_nodes numpy array estimated correlation matrix\n",
    "    '''\n",
    "    sem_est = SemEstimator(data, sparsity=4)\n",
    "\n",
    "    # provide to the estimator the DAG found above\n",
    "    sem_est.pattern = fges_result['graph']\n",
    "\n",
    "    # estimate the weights and residuals\n",
    "    sem_est.estimate()\n",
    "\n",
    "    # get covariance matrix from SemEstimator\n",
    "    est_cov = sem_est.graph_cov\n",
    "\n",
    "    # compute correlation matrix from covariance matrix\n",
    "    stdistdj = np.sqrt(np.diag(est_cov))\n",
    "    est_corr = est_cov / np.outer(stdistdj, stdistdj)\n",
    "    return est_corr\n",
    "\n",
    "est_corr = estimate_corr(data, fges_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the estimated correlation matrix (remember to zero out the diagonal)\n",
    "and compare to the correlation matrix computed from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantify how closely the estimated correlation matrix matches that found\n",
    "from the data. To do this, vectorize the lower-triangular elements in the true\n",
    "and estimated matrices and compute the Pearson correlation between the two vectors.\n",
    "\n",
    "`np.corrcoef` and `np.tril_indices` may be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare graphs found from different subject groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have performed this analysis for samples from one file (one set\n",
    "of 80 subjects), we can compare the results of this pipeline across various\n",
    "data subsets. One option is to compare results across the data files included\n",
    "here (where each file corresponds to a different set of 80 subjects). Another\n",
    "option is to compare results across sample subsets from the session we have \n",
    "worked with so far. Try one or both of these approaches and repurpose the functions \n",
    "above to construct a for loop that for each sample set:\n",
    "\n",
    "1. loads in the data\n",
    "2. runs FGES to get a list of inferred edges\n",
    "3. converts the list of edges to an adjacency matrix\n",
    "\n",
    "Store the adjacency matrix found on each loop iteration in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average adjacency matrix and visualize it with `plt.imshow` to see\n",
    "how often each edge is found from data across sample sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
